{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Strona g\u0142\u00f3wna","text":"<p>Witam Pa\u0144stwa na zaj\u0119ciach z przedmiotu Analiza danych w naukach o Ziemi dla kierunku Geoinformatyka. Poniewa\u017c jest to kurs na studiach magisterskich, chcia\u0142bym, \u017ceby zaj\u0119cie by\u0142y w du\u017cej mierze projektowe.</p>"},{"location":"#informacje-o-przedmocie","title":"\ud83d\udccc Informacje o przedmocie","text":"<ul> <li>Prowadz\u0105cy: Jakub Staszel</li> <li>Forma zaj\u0119\u0107: \u0107wiczenia laboratoryjne</li> <li>Liczba godzin: 1h 30 min x 30</li> <li>Miejsce: Sala 313, A0</li> </ul>"},{"location":"#cel-kursu","title":"\ud83c\udfaf Cel kursu","text":"<p>Po uko\u0144czeniu kursu uczestnicy b\u0119d\u0105 potrafili:</p> <ul> <li>Wykorzystywa\u0107 Python do analizy i przetwarzania danych przestrzennych,</li> <li>Korzysta\u0107 z bibliotek takich jak GeoPandas, Rasterio, Fiona czy Shapely,</li> <li>Tworzy\u0107 interaktywne mapy przy u\u017cyciu Folium i Streamlit,</li> <li>U\u017cywa\u0107 Dask do przetwarzania r\u00f3wnoleg\u0142ego.</li> </ul>"},{"location":"#forma-zaliczenia","title":"\ud83d\udcdd Forma zaliczenia","text":"<p>Aby zaliczy\u0107 kurs, uczestnicy musz\u0105:</p> <ol> <li>Wykona\u0107 projekt semestralny,</li> <li>Doda\u0107 dokumentacj\u0119 projektu w MkDocs,</li> <li>Przedstawi\u0107 dzia\u0142aj\u0105cy projekt na ostatnich zaj\u0119ciach.</li> </ol>"},{"location":"#projekty","title":"\ud83d\ude80 Projekty","text":"<ul> <li>Tematy projekt\u00f3w zostan\u0105 podane w p\u00f3\u017aniejszym terminie,</li> <li>Projekty b\u0119d\u0105 realizowane w grupach 2 lub 3 osobowych,</li> <li>Je\u015bli kto\u015b ma w\u0142asny pomys\u0142 na projekt, mo\u017cna go zrealizowa\u0107 w ramach tych zaj\u0119\u0107.</li> </ul>"},{"location":"#wymagania","title":"\ud83d\udee0\ufe0f Wymagania","text":"<ul> <li>Znajomo\u015b\u0107 podstaw Python, GIS i git,</li> <li>Konto na GitHub.</li> </ul>"},{"location":"#kontakt","title":"\ud83d\udce2 Kontakt","text":"<p>W razie pyta\u0144, prosz\u0119 o kontakt na Teams lub poprzez email - jstaszel@agh.edu.pl.</p>"},{"location":"dask/","title":"Dask","text":"In\u00a0[1]: Copied! <pre>from osgeo import gdal\nimport dask.array as da\nimport rioxarray as rxr\nimport dask\nimport matplotlib.pyplot as plt\nfrom dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler\nfrom dask.distributed import Client\nimport pystac_client\n</pre> from osgeo import gdal import dask.array as da import rioxarray as rxr import dask import matplotlib.pyplot as plt from dask.diagnostics import ProgressBar, Profiler, ResourceProfiler, CacheProfiler from dask.distributed import Client import pystac_client In\u00a0[2]: Copied! <pre># Wyszukiwanie w katalogu STAC\nstac_url = \"https://earth-search.aws.element84.com/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n\n# Filtrujemy obrazy Sentinel-2 na podstawie lokalizacji i daty\nsearch = stac_client.search(\n    collections=[\"sentinel-2-c1-l2a\"],\n    bbox=[69.5, 34.0, 70.5, 35.0],  # Przyk\u0142adowy bbox (Afganistan, zmie\u0144 na swoje wsp\u00f3\u0142rz\u0119dne)\n    datetime=\"2023-01-06T00:00:00Z/2023-06-22T23:59:59Z\",\n    max_items=1\n)\n\nitem = next(search.items(), None)\nif not item:\n    raise ValueError(\"Nie znaleziono pasuj\u0105cych obraz\u00f3w Sentinel-2 w STAC API.\")\n</pre> # Wyszukiwanie w katalogu STAC stac_url = \"https://earth-search.aws.element84.com/v1\" stac_client = pystac_client.Client.open(stac_url)  # Filtrujemy obrazy Sentinel-2 na podstawie lokalizacji i daty search = stac_client.search(     collections=[\"sentinel-2-c1-l2a\"],     bbox=[69.5, 34.0, 70.5, 35.0],  # Przyk\u0142adowy bbox (Afganistan, zmie\u0144 na swoje wsp\u00f3\u0142rz\u0119dne)     datetime=\"2023-01-06T00:00:00Z/2023-06-22T23:59:59Z\",     max_items=1 )  item = next(search.items(), None) if not item:     raise ValueError(\"Nie znaleziono pasuj\u0105cych obraz\u00f3w Sentinel-2 w STAC API.\")  In\u00a0[\u00a0]: Copied! <pre>from dask.distributed import LocalCluster\ncluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit=\"1GB\")\nclient = Client(cluster)\nclient.get_worker_logs()  # Sprawdzenie log\u00f3w worker\u00f3w\ndisplay(client)  # Wy\u015bwietla link do dashboardu\n</pre> from dask.distributed import LocalCluster cluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit=\"1GB\") client = Client(cluster) client.get_worker_logs()  # Sprawdzenie log\u00f3w worker\u00f3w display(client)  # Wy\u015bwietla link do dashboardu Client <p>Client-da49de19-00e9-11f0-a1cc-60a5e265b32c</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>dc93f438</p> Dashboard: http://127.0.0.1:8787/status Workers: 2                  Total threads: 2                  Total memory: 1.86 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-0f5de4c6-1ed4-4508-b4f7-127f590edd7c</p> Comm: tcp://127.0.0.1:63106                      Workers: 2                      Dashboard: http://127.0.0.1:8787/status Total threads: 2                      Started: Just now                      Total memory: 1.86 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:63118                          Total threads:  1                          Dashboard:  http://127.0.0.1:63120/status Memory:  0.93 GiB                          Nanny:  tcp://127.0.0.1:63109                          Local directory:  C:\\Users\\JAS~1.SPY\\AppData\\Local\\Temp\\dask-scratch-space\\worker-3c5ozg8a                          Worker: 1 Comm:  tcp://127.0.0.1:63119                          Total threads:  1                          Dashboard:  http://127.0.0.1:63122/status Memory:  0.93 GiB                          Nanny:  tcp://127.0.0.1:63111                          Local directory:  C:\\Users\\JAS~1.SPY\\AppData\\Local\\Temp\\dask-scratch-space\\worker-ejlwhtsl                          <pre>2025-03-14 18:14:54,471 - distributed.scheduler - WARNING - Worker failed to heartbeat for 828s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:63118', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-14 18:14:54,493 - distributed.scheduler - WARNING - Worker failed to heartbeat for 828s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:63119', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-14 18:14:55,974 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 18:14:56,006 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 20:17:53,366 - distributed.scheduler - WARNING - Worker failed to heartbeat for 7193s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:59107', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-14 20:17:53,369 - distributed.scheduler - WARNING - Worker failed to heartbeat for 7193s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:59108', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-14 20:17:54,916 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 20:17:54,939 - distributed.nanny - WARNING - Restarting worker\n2025-03-17 07:02:47,548 - distributed.scheduler - WARNING - Worker failed to heartbeat for 211473s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:54541', name: 1, status: running, memory: 0, processing: 0&gt;\n2025-03-17 07:02:47,549 - distributed.scheduler - WARNING - Worker failed to heartbeat for 211473s; attempting restart: &lt;WorkerState 'tcp://127.0.0.1:54542', name: 0, status: running, memory: 0, processing: 0&gt;\n2025-03-17 07:02:49,676 - distributed.nanny - WARNING - Restarting worker\n2025-03-17 07:02:49,711 - distributed.nanny - WARNING - Restarting worker\n</pre> In\u00a0[4]: Copied! <pre># Pobranie linku do pasma czerwonego (B04)\ncog_url = item.assets[\"red\"].href\n\nimport rasterio\nfrom rasterio.session import AWSSession\nimport boto3\n\n# Pobierz plik do lokalnego cache\nsession = AWSSession(boto3.Session(), requester_pays=True)\nwith rasterio.Env(session):\n    raster = rxr.open_rasterio(cog_url, masked=False, chunks=\"auto\", overview_level=0, session=session)\nraster = raster.squeeze(dim=\"band\")  # Usuwa wymiar \"band\" je\u015bli ma tylko jeden element\nraster = raster.chunk({\"x\": 512, \"y\": 512}).astype(\"float32\")\n</pre>  # Pobranie linku do pasma czerwonego (B04) cog_url = item.assets[\"red\"].href  import rasterio from rasterio.session import AWSSession import boto3  # Pobierz plik do lokalnego cache session = AWSSession(boto3.Session(), requester_pays=True) with rasterio.Env(session):     raster = rxr.open_rasterio(cog_url, masked=False, chunks=\"auto\", overview_level=0, session=session) raster = raster.squeeze(dim=\"band\")  # Usuwa wymiar \"band\" je\u015bli ma tylko jeden element raster = raster.chunk({\"x\": 512, \"y\": 512}).astype(\"float32\") In\u00a0[5]: Copied! <pre>raster\n</pre> raster Out[5]: <pre>&lt;xarray.DataArray (y: 5490, x: 5490)&gt; Size: 121MB\ndask.array&lt;astype, shape=(5490, 5490), dtype=float32, chunksize=(512, 512), chunktype=numpy.ndarray&gt;\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 44kB 5e+05 5e+05 5e+05 ... 6.098e+05 6.098e+05\n  * y            (y) float64 44kB 3.8e+06 3.8e+06 3.8e+06 ... 3.69e+06 3.69e+06\n    spatial_ref  int64 8B 0\nAttributes:\n    OVR_RESAMPLING_ALG:  AVERAGE\n    AREA_OR_POINT:       Area\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0</pre>xarray.DataArray<ul><li>y: 5490</li><li>x: 5490</li></ul><ul><li>dask.array&lt;chunksize=(512, 512), meta=np.ndarray&gt;  Array   Chunk   Bytes   114.98 MiB   1.00 MiB   Shape   (5490, 5490)   (512, 512)   Dask graph   121 chunks in 5 graph layers   Data type   float32 numpy.ndarray  5490 5490 </li><li>Coordinates: (4)<ul><li>band()int641<pre>array(1)</pre></li><li>x(x)float645e+05 5e+05 ... 6.098e+05 6.098e+05<pre>array([499990., 500010., 500030., ..., 609730., 609750., 609770.],\n      shape=(5490,))</pre></li><li>y(y)float643.8e+06 3.8e+06 ... 3.69e+06<pre>array([3800030., 3800010., 3799990., ..., 3690290., 3690270., 3690250.],\n      shape=(5490,))</pre></li><li>spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 42N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",69],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32642\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 42Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :69.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 42N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",69],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32642\"]]GeoTransform :499980.0 20.0 0.0 3800040.0 0.0 -20.0<pre>array(0)</pre></li></ul></li><li>Indexes: (2)<ul><li>xPandasIndex<pre>PandasIndex(Index([499990.0, 500010.0, 500030.0, 500050.0, 500070.0, 500090.0, 500110.0,\n       500130.0, 500150.0, 500170.0,\n       ...\n       609590.0, 609610.0, 609630.0, 609650.0, 609670.0, 609690.0, 609710.0,\n       609730.0, 609750.0, 609770.0],\n      dtype='float64', name='x', length=5490))</pre></li><li>yPandasIndex<pre>PandasIndex(Index([3800030.0, 3800010.0, 3799990.0, 3799970.0, 3799950.0, 3799930.0,\n       3799910.0, 3799890.0, 3799870.0, 3799850.0,\n       ...\n       3690430.0, 3690410.0, 3690390.0, 3690370.0, 3690350.0, 3690330.0,\n       3690310.0, 3690290.0, 3690270.0, 3690250.0],\n      dtype='float64', name='y', length=5490))</pre></li></ul></li><li>Attributes: (5)OVR_RESAMPLING_ALG :AVERAGEAREA_OR_POINT :Area_FillValue :0scale_factor :1.0add_offset :0.0</li></ul> In\u00a0[6]: Copied! <pre>workers_info = client.scheduler_info()[\"workers\"]\nprint(\"Lista worker\u00f3w Dask:\")\nfor worker, info in workers_info.items():\n    memory_used = info.get(\"metrics\", {}).get(\"memory\", \"Brak danych\")\n    nthreads = info.get(\"nthreads\", \"Brak danych\")\n    print(f\"Worker: {worker}, Pami\u0119\u0107: {memory_used} B, Aktywne w\u0105tki: {nthreads}\")\n</pre> workers_info = client.scheduler_info()[\"workers\"] print(\"Lista worker\u00f3w Dask:\") for worker, info in workers_info.items():     memory_used = info.get(\"metrics\", {}).get(\"memory\", \"Brak danych\")     nthreads = info.get(\"nthreads\", \"Brak danych\")     print(f\"Worker: {worker}, Pami\u0119\u0107: {memory_used} B, Aktywne w\u0105tki: {nthreads}\") <pre>Lista worker\u00f3w Dask:\nWorker: tcp://127.0.0.1:63118, Pami\u0119\u0107: 121352192 B, Aktywne w\u0105tki: 1\nWorker: tcp://127.0.0.1:63119, Pami\u0119\u0107: 121106432 B, Aktywne w\u0105tki: 1\n</pre> In\u00a0[7]: Copied! <pre>from normalize import normalize_xr\n</pre> from normalize import normalize_xr In\u00a0[71]: Copied! <pre>import xarray as xr\n\ndef normalize_xr(data):\n    min_val = data.min()\n    max_val = data.max()\n    return (data - min_val) / (max_val - min_val)\n</pre> import xarray as xr  def normalize_xr(data):     min_val = data.min()     max_val = data.max()     return (data - min_val) / (max_val - min_val) In\u00a0[9]: Copied! <pre>import xarray as xr\n</pre> import xarray as xr  In\u00a0[10]: Copied! <pre>normalized_raster = xr.apply_ufunc(normalize_xr, raster, dask='parallelized', output_dtypes=[raster.dtype])\n</pre> normalized_raster = xr.apply_ufunc(normalize_xr, raster, dask='parallelized', output_dtypes=[raster.dtype]) In\u00a0[12]: Copied! <pre>raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem\n\nnormalized_raster = (raster - raster.min()) / (raster.max() - raster.min())\n</pre> raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem  normalized_raster = (raster - raster.min()) / (raster.max() - raster.min()) <pre>2025-03-14 16:36:35,708 - distributed.protocol.pickle - ERROR - Failed to serialize &lt;ToPickle: HighLevelGraph with 1 layers.\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3de6e8650&gt;\n 0. 2146920665792\n&gt;.\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 60, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 65, in dumps\n    pickler.dump(x)\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 77, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1537, in dumps\n    cp.dump(obj)\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1303, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\n_pickle.PicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:60, in dumps(x, buffer_callback, protocol)\n     59 try:\n---&gt; 60     result = pickle.dumps(x, **dump_kwargs)\n     61 except Exception:\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:65, in dumps(x, buffer_callback, protocol)\n     64 buffers.clear()\n---&gt; 65 pickler.dump(x)\n     66 result = f.getvalue()\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nPicklingError                             Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:366, in serialize(x, serializers, on_error, context, iterate_collection)\n    365 try:\n--&gt; 366     header, frames = dumps(x, context=context) if wants_context else dumps(x)\n    367     header[\"serializer\"] = name\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:78, in pickle_dumps(x, context)\n     76     writeable.append(not f.readonly)\n---&gt; 78 frames[0] = pickle.dumps(\n     79     x,\n     80     buffer_callback=buffer_callback,\n     81     protocol=context.get(\"pickle-protocol\", None) if context else None,\n     82 )\n     83 header = {\n     84     \"serializer\": \"pickle\",\n     85     \"writeable\": tuple(writeable),\n     86 }\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:77, in dumps(x, buffer_callback, protocol)\n     76     buffers.clear()\n---&gt; 77     result = cloudpickle.dumps(x, **dump_kwargs)\n     78 except Exception:\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1537, in dumps(obj, protocol, buffer_callback)\n   1536 cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n-&gt; 1537 cp.dump(obj)\n   1538 return file.getvalue()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1303, in Pickler.dump(self, obj)\n   1302 try:\n-&gt; 1303     return super().dump(obj)\n   1304 except RuntimeError as e:\n\nPicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 raster = raster.compute()  # Pobiera dane do RAM przed dalszym przetwarzaniem\n      3 normalized_raster = (raster - raster.min()) / (raster.max() - raster.min())\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1206, in DataArray.compute(self, **kwargs)\n   1181 \"\"\"Manually trigger loading of this array's data from disk or a\n   1182 remote source into memory and return a new array.\n   1183 \n   (...)   1203 dask.compute\n   1204 \"\"\"\n   1205 new = self.copy(deep=False)\n-&gt; 1206 return new.load(**kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1174, in DataArray.load(self, **kwargs)\n   1154 def load(self, **kwargs) -&gt; Self:\n   1155     \"\"\"Manually trigger loading of this array's data from disk or a\n   1156     remote source into memory and return this array.\n   1157 \n   (...)   1172     dask.compute\n   1173     \"\"\"\n-&gt; 1174     ds = self._to_temp_dataset().load(**kwargs)\n   1175     new = self._from_temp_dataset(ds)\n   1176     self._variable = new._variable\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\dask\\base.py:662, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659     postcomputes.append(x.__dask_postcompute__())\n    661 with shorten_traceback():\n--&gt; 662     results = schedule(dsk, keys, **kwargs)\n    664 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:392, in serialize(x, serializers, on_error, context, iterate_collection)\n    390     except Exception:\n    391         raise TypeError(msg) from exc\n--&gt; 392     raise TypeError(msg, str_x) from exc\n    393 else:  # pragma: nocover\n    394     raise ValueError(f\"{on_error=}; expected 'message' or 'raise'\")\n\nTypeError: ('Could not serialize object of type HighLevelGraph', '&lt;ToPickle: HighLevelGraph with 1 layers.\\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3de6e8650&gt;\\n 0. 2146920665792\\n&gt;')</pre> In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 6))\n\nwith Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():\n    result = normalized_raster.compute()\n</pre> fig, ax = plt.subplots(figsize=(8, 6))  with Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():     result = normalized_raster.compute() <pre>2025-03-14 16:35:21,121 - distributed.protocol.pickle - ERROR - Failed to serialize &lt;ToPickle: HighLevelGraph with 1 layers.\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3dbc91f90&gt;\n 0. 2146875929152\n&gt;.\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 60, in dumps\n    result = pickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 65, in dumps\n    pickler.dump(x)\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py\", line 77, in dumps\n    result = cloudpickle.dumps(x, **dump_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1537, in dumps\n    cp.dump(obj)\n  File \"c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py\", line 1303, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\n_pickle.PicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:60, in dumps(x, buffer_callback, protocol)\n     59 try:\n---&gt; 60     result = pickle.dumps(x, **dump_kwargs)\n     61 except Exception:\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nAttributeError                            Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:65, in dumps(x, buffer_callback, protocol)\n     64 buffers.clear()\n---&gt; 65 pickler.dump(x)\n     66 result = f.getvalue()\n\nAttributeError: Can't pickle local object 'lazy_call.&lt;locals&gt;._handler'\n\nDuring handling of the above exception, another exception occurred:\n\nPicklingError                             Traceback (most recent call last)\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:366, in serialize(x, serializers, on_error, context, iterate_collection)\n    365 try:\n--&gt; 366     header, frames = dumps(x, context=context) if wants_context else dumps(x)\n    367     header[\"serializer\"] = name\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:78, in pickle_dumps(x, context)\n     76     writeable.append(not f.readonly)\n---&gt; 78 frames[0] = pickle.dumps(\n     79     x,\n     80     buffer_callback=buffer_callback,\n     81     protocol=context.get(\"pickle-protocol\", None) if context else None,\n     82 )\n     83 header = {\n     84     \"serializer\": \"pickle\",\n     85     \"writeable\": tuple(writeable),\n     86 }\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\pickle.py:77, in dumps(x, buffer_callback, protocol)\n     76     buffers.clear()\n---&gt; 77     result = cloudpickle.dumps(x, **dump_kwargs)\n     78 except Exception:\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1537, in dumps(obj, protocol, buffer_callback)\n   1536 cp = Pickler(file, protocol=protocol, buffer_callback=buffer_callback)\n-&gt; 1537 cp.dump(obj)\n   1538 return file.getvalue()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1303, in Pickler.dump(self, obj)\n   1302 try:\n-&gt; 1303     return super().dump(obj)\n   1304 except RuntimeError as e:\n\nPicklingError: Can't pickle &lt;built-in function input&gt;: it's not the same object as builtins.input\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[11], line 4\n      1 fig, ax = plt.subplots(figsize=(8, 6))\n      3 with Profiler() as prof, ResourceProfiler() as rprof, CacheProfiler() as cprof, ProgressBar():\n----&gt; 4     result = normalized_raster.compute()\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1206, in DataArray.compute(self, **kwargs)\n   1181 \"\"\"Manually trigger loading of this array's data from disk or a\n   1182 remote source into memory and return a new array.\n   1183 \n   (...)   1203 dask.compute\n   1204 \"\"\"\n   1205 new = self.copy(deep=False)\n-&gt; 1206 return new.load(**kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataarray.py:1174, in DataArray.load(self, **kwargs)\n   1154 def load(self, **kwargs) -&gt; Self:\n   1155     \"\"\"Manually trigger loading of this array's data from disk or a\n   1156     remote source into memory and return this array.\n   1157 \n   (...)   1172     dask.compute\n   1173     \"\"\"\n-&gt; 1174     ds = self._to_temp_dataset().load(**kwargs)\n   1175     new = self._from_temp_dataset(ds)\n   1176     self._variable = new._variable\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\core\\dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\dask\\base.py:662, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    659     postcomputes.append(x.__dask_postcompute__())\n    661 with shorten_traceback():\n--&gt; 662     results = schedule(dsk, keys, **kwargs)\n    664 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile c:\\ProgramData\\miniforge3\\envs\\daes-env\\Lib\\site-packages\\distributed\\protocol\\serialize.py:392, in serialize(x, serializers, on_error, context, iterate_collection)\n    390     except Exception:\n    391         raise TypeError(msg) from exc\n--&gt; 392     raise TypeError(msg, str_x) from exc\n    393 else:  # pragma: nocover\n    394     raise ValueError(f\"{on_error=}; expected 'message' or 'raise'\")\n\nTypeError: ('Could not serialize object of type HighLevelGraph', '&lt;ToPickle: HighLevelGraph with 1 layers.\\n&lt;dask.highlevelgraph.HighLevelGraph object at 0x1f3dbc91f90&gt;\\n 0. 2146875929152\\n&gt;')</pre> In\u00a0[\u00a0]: Copied! <pre>ax.imshow(result, cmap=\"gray\")\nax.set_title(\"Znormalizowane pasmo czerwone Sentinel-2\")\nplt.show()\n</pre> ax.imshow(result, cmap=\"gray\") ax.set_title(\"Znormalizowane pasmo czerwone Sentinel-2\") plt.show() In\u00a0[\u00a0]: Copied! <pre>display(prof.visualize())  # Czas wykonania poszczeg\u00f3lnych operacji\ndisplay(rprof.visualize()) # U\u017cycie CPU i pami\u0119ci\n</pre> display(prof.visualize())  # Czas wykonania poszczeg\u00f3lnych operacji display(rprof.visualize()) # U\u017cycie CPU i pami\u0119ci"},{"location":"dask/#dask","title":"Dask\u00b6","text":"<p>Dask to biblioteka, kt\u00f3ra umo\u017cliwia r\u00f3wnoleg\u0142e przetwarzanie du\u017cych zbior\u00f3w danych, rozdzielaj\u0105c je na mniejsze fragmenty i przetwarzaj\u0105c je r\u00f3wnocze\u015bnie. Jest szczeg\u00f3lnie przydatna w analizie danych przestrzennych, gdzie pliki mog\u0105 by\u0107 zbyt du\u017ce, by zmie\u015bci\u0107 si\u0119 w pami\u0119ci RAM.</p> <p>Zalety Daska:</p> <ul> <li>Przetwarzanie r\u00f3wnoleg\u0142e \u2013 dzia\u0142a na wielu rdzeniach CPU lub w klastrze obliczeniowym.</li> <li>Dynamiczne skalowanie \u2013 mo\u017cna pracowa\u0107 zar\u00f3wno na laptopie, jak i w \u015brodowisku rozproszonym.</li> <li>Interoperacyjno\u015b\u0107 \u2013 wsp\u00f3\u0142pracuje z Pandas, Xarray, NumPy, a tak\u017ce narz\u0119dziami GIS jak rasterio czy rioxarray.</li> <li>Lazy evaluation \u2013 nie wykonuje oblicze\u0144 od razu, tylko buduje graf zada\u0144, kt\u00f3ry mo\u017cna uruchomi\u0107 dopiero, gdy jest to potrzebne.</li> </ul>"},{"location":"dask/#rioxarray","title":"Rioxarray\u00b6","text":"<p>Rioxarray to rozszerzenie dla xarray, kt\u00f3re dodaje wsparcie dla georeferencyjnych danych rastrowych. Umo\u017cliwia:</p> <ul> <li>\u0142atwe wczytywanie i zapisywanie plik\u00f3w GeoTIFF,</li> <li>reprojekcj\u0119 i analiz\u0119 przestrzenn\u0105,</li> <li>interakcj\u0119 z innymi narz\u0119dziami GIS (Rasterio, GDAL).</li> </ul> <p>Rioxarray \u015bwietnie integruje si\u0119 z Daskiem, umo\u017cliwiaj\u0105c przetwarzanie ogromnych zbior\u00f3w danych rastrowych w spos\u00f3b efektywny pami\u0119ciowo.</p>"},{"location":"dask/#przeszukiwanie-stac","title":"Przeszukiwanie STAC\u00b6","text":""},{"location":"dask/#inicjalizacja-klienta-dask","title":"Inicjalizacja klienta Dask\u00b6","text":""},{"location":"dask/#pobranie-pliku","title":"Pobranie pliku\u00b6","text":""},{"location":"dask/#podglad-workerow","title":"Podgl\u0105d worker\u00f3w\u00b6","text":""},{"location":"dask/#normalizacja-obrazu","title":"Normalizacja obrazu\u00b6","text":""},{"location":"dask/#uruchomienie-przetwarzania","title":"Uruchomienie przetwarzania\u00b6","text":""},{"location":"dask/#wizualizacja","title":"Wizualizacja\u00b6","text":""},{"location":"dask/#analiza-profilowania","title":"Analiza profilowania\u00b6","text":""},{"location":"geoparquet/","title":"\ud83d\udc7d GeoParquet","text":"In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\nimport folium\nfrom shapely.geometry import box\nfrom fsspec.implementations.http import HTTPFileSystem\nimport pyarrow.parquet as pq\nfrom geopandas.io.arrow import _arrow_to_geopandas\nfrom shapely.geometry import box\nfrom folium.plugins import HeatMap\n</pre> import geopandas as gpd import folium from shapely.geometry import box from fsspec.implementations.http import HTTPFileSystem import pyarrow.parquet as pq from geopandas.io.arrow import _arrow_to_geopandas from shapely.geometry import box from folium.plugins import HeatMap In\u00a0[\u00a0]: Copied! <pre>file_url = \"https://data.source.coop/cholmes/eurocrops/unprojected/geoparquet/FR_2018_EC21.parquet\"\nfilesystem = HTTPFileSystem()\n</pre> file_url = \"https://data.source.coop/cholmes/eurocrops/unprojected/geoparquet/FR_2018_EC21.parquet\" filesystem = HTTPFileSystem() <p>To tylko wczytuje metadane</p> In\u00a0[\u00a0]: Copied! <pre>parquet_file = pq.ParquetFile(file_url, filesystem=filesystem)\nprint(f\"Columns: {parquet_file.schema_arrow.names}\")\nprint(f\"Number of rows: {parquet_file.metadata.num_rows}\")\nprint(f\"Number of row groups: {parquet_file.num_row_groups}\")\n</pre> parquet_file = pq.ParquetFile(file_url, filesystem=filesystem) print(f\"Columns: {parquet_file.schema_arrow.names}\") print(f\"Number of rows: {parquet_file.metadata.num_rows}\") print(f\"Number of row groups: {parquet_file.num_row_groups}\") <p>Tutaj wczytujemy faktycznie dane, ale tylko dla jednej z grup wierszy</p> In\u00a0[\u00a0]: Copied! <pre>pyarrow_table = parquet_file.read_row_group(0, columns=[\"ID_PARCEL\", \"SURF_PARC\", \"geometry\"])\ngeopandas_gdf = _arrow_to_geopandas(pyarrow_table)\n</pre> pyarrow_table = parquet_file.read_row_group(0, columns=[\"ID_PARCEL\", \"SURF_PARC\", \"geometry\"]) geopandas_gdf = _arrow_to_geopandas(pyarrow_table) In\u00a0[\u00a0]: Copied! <pre>geopandas_gdf.head(10)\n</pre> geopandas_gdf.head(10) In\u00a0[\u00a0]: Copied! <pre># Przekszta\u0142cenie do EPSG:4326 je\u015bli dane s\u0105 w innym uk\u0142adzie\nif geopandas_gdf.crs.to_epsg() != 4326:\n    geopandas_gdf = geopandas_gdf.to_crs(epsg=4326)\n</pre> # Przekszta\u0142cenie do EPSG:4326 je\u015bli dane s\u0105 w innym uk\u0142adzie if geopandas_gdf.crs.to_epsg() != 4326:     geopandas_gdf = geopandas_gdf.to_crs(epsg=4326) In\u00a0[\u00a0]: Copied! <pre># Usuni\u0119cie pustych i nieprawid\u0142owych geometrii\ngeopandas_gdf = geopandas_gdf[~geopandas_gdf.geometry.is_empty &amp; geopandas_gdf.geometry.notnull()]\n\n# Naprawa geometrii, aby unikn\u0105\u0107 b\u0142\u0119d\u00f3w topologicznych\ngeopandas_gdf['geometry'] = geopandas_gdf['geometry'].buffer(0)\n</pre> # Usuni\u0119cie pustych i nieprawid\u0142owych geometrii geopandas_gdf = geopandas_gdf[~geopandas_gdf.geometry.is_empty &amp; geopandas_gdf.geometry.notnull()]  # Naprawa geometrii, aby unikn\u0105\u0107 b\u0142\u0119d\u00f3w topologicznych geopandas_gdf['geometry'] = geopandas_gdf['geometry'].buffer(0) In\u00a0[\u00a0]: Copied! <pre># Definicja granic Francji w EPSG:4326\nfrance_bbox = box(-5.0, 41.0, 9.7, 51.1)\n\n# Usuni\u0119cie geometrii znajduj\u0105cych si\u0119 poza granicami Francji\ngeopandas_gdf = geopandas_gdf[geopandas_gdf.geometry.within(france_bbox)]\n</pre> # Definicja granic Francji w EPSG:4326 france_bbox = box(-5.0, 41.0, 9.7, 51.1)  # Usuni\u0119cie geometrii znajduj\u0105cych si\u0119 poza granicami Francji geopandas_gdf = geopandas_gdf[geopandas_gdf.geometry.within(france_bbox)] In\u00a0[\u00a0]: Copied! <pre># Konwersja kolumny SURF_PARC do float, je\u015bli zawiera decimal.Decimal\ngeopandas_gdf['SURF_PARC'] = geopandas_gdf['SURF_PARC'].astype(float)\n\n# Obliczenie progu dla 10% najwi\u0119kszych warto\u015bci powierzchni\nthreshold = geopandas_gdf['SURF_PARC'].quantile(0.9)\n</pre> # Konwersja kolumny SURF_PARC do float, je\u015bli zawiera decimal.Decimal geopandas_gdf['SURF_PARC'] = geopandas_gdf['SURF_PARC'].astype(float)  # Obliczenie progu dla 10% najwi\u0119kszych warto\u015bci powierzchni threshold = geopandas_gdf['SURF_PARC'].quantile(0.9) In\u00a0[\u00a0]: Copied! <pre># Filtrowanie 10% najwi\u0119kszych parceli\ngdf_top_10 = geopandas_gdf[geopandas_gdf['SURF_PARC'] &gt;= threshold]\n</pre> # Filtrowanie 10% najwi\u0119kszych parceli gdf_top_10 = geopandas_gdf[geopandas_gdf['SURF_PARC'] &gt;= threshold] In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy interaktywnej\nm = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)\n\n# Dodanie warstwy polygon\u00f3w\ndef add_gdf_to_map(gdf, fmap):\n    for _, row in gdf.iterrows():\n        folium.GeoJson(row.geometry).add_to(fmap)\n\nadd_gdf_to_map(gdf_top_10, m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy interaktywnej m = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)  # Dodanie warstwy polygon\u00f3w def add_gdf_to_map(gdf, fmap):     for _, row in gdf.iterrows():         folium.GeoJson(row.geometry).add_to(fmap)  add_gdf_to_map(gdf_top_10, m)  # Wy\u015bwietlenie mapy m In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy heatmap\nheatmap_data = [[point.y, point.x] for point in geopandas_gdf.geometry.centroid]\n\nm_heatmap = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10)\nHeatMap(heatmap_data).add_to(m_heatmap)\n\n# Wy\u015bwietlenie mapy heatmap\nm_heatmap\n</pre> # Tworzenie mapy heatmap heatmap_data = [[point.y, point.x] for point in geopandas_gdf.geometry.centroid]  m_heatmap = folium.Map(location=[gdf_top_10.geometry.centroid.y.mean(), gdf_top_10.geometry.centroid.x.mean()], zoom_start=10) HeatMap(heatmap_data).add_to(m_heatmap)  # Wy\u015bwietlenie mapy heatmap m_heatmap"},{"location":"geoparquet/#geoparquet","title":"GeoParquet\u00b6","text":"<p>GeoParquet to format zoptymalizowany do przechowywania i przetwarzania danych geoprzestrzennych.</p> <ul> <li>Umo\u017cliwia szybkie odczytywanie i zapisywanie du\u017cych zbior\u00f3w danych dzi\u0119ki kolumnowej strukturze.</li> <li>Obs\u0142uguje r\u00f3wnoleg\u0142e przetwarzanie, co czyni go idealnym do analizy du\u017cych zbior\u00f3w danych.</li> <li>Umo\u017cliwia efektywne filtrowanie i kompresj\u0119, zmniejszaj\u0105c ilo\u015b\u0107 przesy\u0142anych i zapisywanych danych.</li> <li>W przeciwie\u0144stwie do format\u00f3w takich jak Shapefile czy GeoJSON, pozwala na przechowywanie atrybut\u00f3w o r\u00f3\u017cnych typach danych bez ogranicze\u0144.</li> </ul>"},{"location":"geoparquet/#wczytanie-pliku-geoparquet-do-geodataframe-i-podglad-danych","title":"Wczytanie pliku GeoParquet do GeoDataFrame i podgl\u0105d danych\u00b6","text":"<p>GeoPandas to rozszerzenie dla Pandas, kt\u00f3re umo\u017cliwia prac\u0119 z danymi przestrzennymi w Pythonie. U\u0142atwia analiz\u0119 geometrii (punkty, linie, poligony), przekszta\u0142canie uk\u0142ad\u00f3w wsp\u00f3\u0142rz\u0119dnych, operacje przestrzenne (przeci\u0119cia, \u0142\u0105czenia, buforowanie) oraz wizualizacj\u0119 na mapach.</p> <p>GeoPandas wykorzystuje Shapely do obs\u0142ugi geometrii, Fiona do wczytywania danych wektorowych, a Matplotlib do prostych wizualizacji.</p>"},{"location":"geoparquet/#naprawianie-geometrii","title":"Naprawianie geometrii\u00b6","text":""},{"location":"geoparquet/#filtrowanie","title":"Filtrowanie\u00b6","text":""},{"location":"geoparquet/#mapa-interaktywna","title":"Mapa interaktywna\u00b6","text":""},{"location":"geoparquet/#heatmap","title":"Heatmap\u00b6","text":""},{"location":"stac/","title":"\ud83d\udc7d STAC","text":"In\u00a0[\u00a0]: Copied! <pre>import pystac_client\nimport planetary_computer as pc\nimport rasterio\nimport numpy as np\nimport folium\nfrom folium.raster_layers import ImageOverlay\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom rasterio.mask import mask\nfrom rasterio.warp import transform_geom\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport io\nimport base64\n</pre> import pystac_client import planetary_computer as pc import rasterio import numpy as np import folium from folium.raster_layers import ImageOverlay import matplotlib.pyplot as plt from PIL import Image from rasterio.mask import mask from rasterio.warp import transform_geom import matplotlib.pyplot as plt import matplotlib.colors as mcolors import io import base64 In\u00a0[\u00a0]: Copied! <pre># Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer\nstac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\nstac_client = pystac_client.Client.open(stac_url)\n\n# Lista dost\u0119pnych kolekcji\ncollections = stac_client.get_all_collections()\nprint(\"Dost\u0119pne kolekcje:\")\nfor collection in collections:\n    print(collection.id, \"-\", collection.title)\n</pre> # Po\u0142\u0105czenie z publicznym katalogiem STAC na Azure Planetary Computer stac_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\" stac_client = pystac_client.Client.open(stac_url)  # Lista dost\u0119pnych kolekcji collections = stac_client.get_all_collections() print(\"Dost\u0119pne kolekcje:\") for collection in collections:     print(collection.id, \"-\", collection.title) In\u00a0[\u00a0]: Copied! <pre># Definiowanie obszaru zainteresowania i przedzia\u0142u czasowego\naoi = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [19.86, 50.02], [20.03, 50.02], [20.03, 50.12], [19.86, 50.12], [19.86, 50.02]\n    ]]\n}\ntime_range = \"2023-06-01/2023-06-30\"\n\n# Wyszukiwanie danych Sentinel-2 spe\u0142niaj\u0105cych kryteria\nsearch = stac_client.search(\n    collections=[\"sentinel-2-l2a\"],\n    intersects=aoi,\n    datetime=time_range,\n    max_items=5\n)\n\nitems = list(search.items())\nprint(f\"Znaleziono {len(items)} scen Sentinel-2\")\nprint(f\"Nazwy scen: {items}\")\n</pre> # Definiowanie obszaru zainteresowania i przedzia\u0142u czasowego aoi = {     \"type\": \"Polygon\",     \"coordinates\": [[         [19.86, 50.02], [20.03, 50.02], [20.03, 50.12], [19.86, 50.12], [19.86, 50.02]     ]] } time_range = \"2023-06-01/2023-06-30\"  # Wyszukiwanie danych Sentinel-2 spe\u0142niaj\u0105cych kryteria search = stac_client.search(     collections=[\"sentinel-2-l2a\"],     intersects=aoi,     datetime=time_range,     max_items=5 )  items = list(search.items()) print(f\"Znaleziono {len(items)} scen Sentinel-2\") print(f\"Nazwy scen: {items}\")  In\u00a0[\u00a0]: Copied! <pre>item = items[0]\n# Pobranie pasm B04 (czerwone) i B08 (NIR)\nb04_asset = pc.sign(item.assets[\"B04\"].href)\nb08_asset = pc.sign(item.assets[\"B08\"].href)\n</pre> item = items[0] # Pobranie pasm B04 (czerwone) i B08 (NIR) b04_asset = pc.sign(item.assets[\"B04\"].href) b08_asset = pc.sign(item.assets[\"B08\"].href) In\u00a0[\u00a0]: Copied! <pre># Przyci\u0119cie zobrazowania do AOI\ndef clip_raster(dataset, aoi):\n    from shapely.geometry import shape\n    import json\n    \n    # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra\n    aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))\n    \n    aoi_geom = [json.loads(json.dumps(aoi_transformed))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)\n    return clipped_array[0], clipped_transform\n    from shapely.geometry import shape\n    import json\n    aoi_geom = [json.loads(json.dumps(aoi))]\n    clipped_array, clipped_transform = mask(dataset, [shape(aoi)], crop=True)\n    return clipped_array[0], clipped_transform\n\n# Otwieranie i przycinanie pasm\nwith rasterio.open(b04_asset) as red_ds, rasterio.open(b08_asset) as nir_ds:\n    red, red_transform = clip_raster(red_ds, aoi)\n    nir, nir_transform = clip_raster(nir_ds, aoi)\n    red = red.astype(np.float32)\n    nir = nir.astype(np.float32)\n    ndvi = (nir - red) / (nir + red + 1e-10)\n    from rasterio.warp import transform_bounds\n\n# Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku\ndataset_crs = red_ds.crs\n\n# Przekszta\u0142cenie granic obrazu do WGS84\nheight, width = red.shape\n# Obliczenie granic po przyci\u0119ciu\nleft, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g\nright, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g\nbounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top)\n</pre> # Przyci\u0119cie zobrazowania do AOI def clip_raster(dataset, aoi):     from shapely.geometry import shape     import json          # Konwersja AOI do uk\u0142adu wsp\u00f3\u0142rz\u0119dnych rastra     aoi_transformed = transform_geom('EPSG:4326', dataset.crs, shape(aoi))          aoi_geom = [json.loads(json.dumps(aoi_transformed))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi_transformed)], crop=True)     return clipped_array[0], clipped_transform     from shapely.geometry import shape     import json     aoi_geom = [json.loads(json.dumps(aoi))]     clipped_array, clipped_transform = mask(dataset, [shape(aoi)], crop=True)     return clipped_array[0], clipped_transform  # Otwieranie i przycinanie pasm with rasterio.open(b04_asset) as red_ds, rasterio.open(b08_asset) as nir_ds:     red, red_transform = clip_raster(red_ds, aoi)     nir, nir_transform = clip_raster(nir_ds, aoi)     red = red.astype(np.float32)     nir = nir.astype(np.float32)     ndvi = (nir - red) / (nir + red + 1e-10)     from rasterio.warp import transform_bounds  # Pobranie uk\u0142adu wsp\u00f3\u0142rz\u0119dnych pliku dataset_crs = red_ds.crs  # Przekszta\u0142cenie granic obrazu do WGS84 height, width = red.shape # Obliczenie granic po przyci\u0119ciu left, top = red_transform * (0, 0)  # Lewy g\u00f3rny r\u00f3g right, bottom = red_transform * (width, height)  # Prawy dolny r\u00f3g bounds = transform_bounds(dataset_crs, 'EPSG:4326', left, bottom, right, top) In\u00a0[\u00a0]: Copied! <pre># Normalizacja do zakresu 0-255\n# Tworzenie mapy kolor\u00f3w do wizualizacji NDVI\ncmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony\nnorm = mcolors.Normalize(vmin=-1, vmax=1)\nndvi_colored = cmap(norm(ndvi))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa\nndvi_colored = (ndvi_colored * 255).astype(np.uint8)\n\n# Tworzenie obrazu NDVI\nimage = Image.fromarray(ndvi_colored, mode=\"RGB\")\nimage = image.convert(\"RGBA\")\n\n# Konwersja obrazu na format base64\nimage_buffer = io.BytesIO()\nimage.save(image_buffer, format='PNG')\nimage_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8')\nimport tempfile\n\n# Zapisanie obrazu NDVI do pliku tymczasowego\ntemp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\nimage.save(temp_file.name, format='PNG')\nimage_url = temp_file.name\n</pre> # Normalizacja do zakresu 0-255 # Tworzenie mapy kolor\u00f3w do wizualizacji NDVI cmap = plt.get_cmap('RdYlGn')  # Czerwony-\u017c\u00f3\u0142ty-zielony norm = mcolors.Normalize(vmin=-1, vmax=1) ndvi_colored = cmap(norm(ndvi))[:, :, :3]  # Usuni\u0119cie kana\u0142u alfa ndvi_colored = (ndvi_colored * 255).astype(np.uint8)  # Tworzenie obrazu NDVI image = Image.fromarray(ndvi_colored, mode=\"RGB\") image = image.convert(\"RGBA\")  # Konwersja obrazu na format base64 image_buffer = io.BytesIO() image.save(image_buffer, format='PNG') image_data = base64.b64encode(image_buffer.getvalue()).decode('utf-8') import tempfile  # Zapisanie obrazu NDVI do pliku tymczasowego temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False) image.save(temp_file.name, format='PNG') image_url = temp_file.name In\u00a0[\u00a0]: Copied! <pre># Tworzenie mapy i dodanie NDVI jako warstwy rastrowej\nminx, miny, maxx, maxy = bounds\nm = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10)\nimage_overlay = ImageOverlay(\n    image=image_url,\n    bounds=[[miny, minx], [maxy, maxx]],\n    opacity=0.6,\n    name=\"NDVI Layer\"\n)\nimage_overlay.add_to(m)\n\n# Dodanie opcji sterowania warstwami\nfolium.LayerControl().add_to(m)\n\n# Wy\u015bwietlenie mapy\nm\n</pre> # Tworzenie mapy i dodanie NDVI jako warstwy rastrowej minx, miny, maxx, maxy = bounds m = folium.Map(location=[(miny + maxy) / 2, (minx + maxx) / 2], zoom_start=10) image_overlay = ImageOverlay(     image=image_url,     bounds=[[miny, minx], [maxy, maxx]],     opacity=0.6,     name=\"NDVI Layer\" ) image_overlay.add_to(m)  # Dodanie opcji sterowania warstwami folium.LayerControl().add_to(m)  # Wy\u015bwietlenie mapy m"},{"location":"stac/#stac","title":"STAC\u00b6","text":"<p>SpatioTemporal Asset Catalog (STAC) to otwarty standard s\u0142u\u017c\u0105cy do organizowania i przeszukiwania danych geoprzestrzennych, takich jak zobrazowania satelitarne, modele terenu czy dane LiDAR. STAC zosta\u0142 zaprojektowany z my\u015bl\u0105 o efektywnym katalogowaniu zasob\u00f3w przestrzennych, co pozwala na ich \u0142atwe odkrywanie, filtrowanie i analiz\u0119. Dzi\u0119ki STAC u\u017cytkownicy mog\u0105 szybko przeszukiwa\u0107 ogromne zbiory danych, stosuj\u0105c kryteria takie jak lokalizacja, przedzia\u0142 czasowy, rozdzielczo\u015b\u0107 czy dost\u0119pno\u015b\u0107 pasm spektralnych.</p> <p>Najwa\u017cniejsze zalety STAC:</p> <ul> <li>Standaryzacja \u2013 jednolity format umo\u017cliwia interoperacyjno\u015b\u0107 mi\u0119dzy r\u00f3\u017cnymi dostawcami danych.</li> <li>Elastyczno\u015b\u0107 \u2013 mo\u017cliwo\u015b\u0107 rozszerzania schemat\u00f3w metadanych i dostosowywania ich do specyficznych potrzeb.</li> <li>\u0141atwo\u015b\u0107 przeszukiwania \u2013 szybkie zapytania pozwalaj\u0105 na odnalezienie interesuj\u0105cych zobrazowa\u0144 na podstawie obszaru i daty.</li> <li>Integracja z chmur\u0105 \u2013 wiele publicznych katalog\u00f3w STAC dost\u0119pnych jest w chmurze, co u\u0142atwia prac\u0119 z du\u017cymi zbiorami danych.</li> </ul> <p>Dzi\u0119ki STAC eksploracja i analiza danych geoprzestrzennych staje si\u0119 znacznie bardziej efektywna, co ma kluczowe znaczenie dla naukowc\u00f3w, analityk\u00f3w oraz firm zajmuj\u0105cych si\u0119 obserwacj\u0105 Ziemi.</p>"},{"location":"stac/#przeszukanie-kolekcji-dostepnych-w-stac-w-planetary-computer","title":"Przeszukanie kolekcji dost\u0119pnych w STAC w Planetary Computer\u00b6","text":""},{"location":"stac/#przeszukiwanie-w-czasie-i-przestrzeni","title":"Przeszukiwanie w czasie i przestrzeni\u00b6","text":""},{"location":"stac/#przeprowadzanie-obliczen","title":"Przeprowadzanie oblicze\u0144\u00b6","text":""},{"location":"stac/#wyswietlanie-wynikow","title":"Wy\u015bwietlanie wynik\u00f3w\u00b6","text":""},{"location":"my_project/desc/","title":"\ud83d\udccb Opis","text":""},{"location":"my_project/desc/#cel","title":"Cel","text":""},{"location":"my_project/desc/#metody","title":"Metody","text":""},{"location":"my_project/desc/#techniczna-implementacja","title":"Techniczna implementacja","text":""},{"location":"my_project/desc/#_1","title":"...","text":""},{"location":"my_project/log/","title":"Log","text":""},{"location":"my_project/log/#28032025","title":"28.03.2025","text":"<p>Post\u0119p:</p> <p>Problemy / kwestie do przedyskutowania:</p>"},{"location":"my_project/log/#21032025","title":"21.03.2025","text":"<p>Post\u0119p:</p> <p>Problemy / kwestie do przedyskutowania:</p>"},{"location":"projects/ambulances/","title":"Analiza przestrzenna dla przejazd\u00f3w karetek w Ma\u0142opolsce","text":"<p>Opis problemu badawczego: System ratownictwa medycznego odgrywa kluczow\u0105 rol\u0119 w zapewnianiu szybkiej pomocy w nag\u0142ych wypadkach. Analiza tras przejazd\u00f3w karetek pozwala na identyfikacj\u0119 wzorc\u00f3w czasowych i przestrzennych, ocen\u0119 efektywno\u015bci dojazd\u00f3w oraz wskazanie obszar\u00f3w wymagaj\u0105cych usprawnienia. Celem projektu jest eksploracyjna analiza przejazd\u00f3w karetek w wojew\u00f3dztwie ma\u0142opolskim na podstawie danych GPS oraz badanie czynnik\u00f3w wp\u0142ywaj\u0105cych na czas reakcji s\u0142u\u017cb ratunkowych.</p> <p>\u0179r\u00f3d\u0142o danych - od prowadz\u0105cego</p> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pna analiza danych GPS<ul> <li>Oczyszczenie i agregacja danych GPS dla poszczeg\u00f3lnych kurs\u00f3w</li> <li>Sprawdzenie kompletno\u015bci danych (np. braki w sygnale, nieprawid\u0142owe punkty)</li> </ul> </li> <li>Analiza czasowa i przestrzenna przejazd\u00f3w<ul> <li>\u015arednie czasy dojazdu do poszczeg\u00f3lnych miejscowo\u015bci / na poszczeg\u00f3lnych obszarach / w siatce H3</li> <li>Analiza godzin szczytu \u2013 czy w okre\u015blonych porach dnia wyst\u0119puj\u0105 wi\u0119ksze op\u00f3\u017anienia?</li> <li>Wizualizacja g\u0119sto\u015bci przejazd\u00f3w \u2013 heatmapy najcz\u0119\u015bciej ucz\u0119szczanych tras.</li> </ul> </li> <li>Wykrywanie anomalii i problematycznych obszar\u00f3w<ul> <li>Identyfikacja tras o nienaturalnie d\u0142ugich czasach przejazdu (np. korki, z\u0142a infrastruktura).</li> <li>Wykrycie cz\u0119sto omijanych obszar\u00f3w \u2013 mo\u017cliwe luki w pokryciu ratowniczym.</li> <li>Analiza wp\u0142ywu teren\u00f3w zurbanizowanych vs wiejskich na efektywno\u015b\u0107 dojazdu.</li> </ul> </li> <li>?Modelowanie predykcyjne czasu dojazdu?<ul> <li>Regresja liniowa/random forest \u2013 przewidywanie czasu przejazdu na podstawie odleg\u0142o\u015bci, godziny, warunk\u00f3w drogowych.</li> <li>Modelowanie szereg\u00f3w czasowych (ARIMA, Prophet) \u2013 przewidywanie nat\u0119\u017cenia przejazd\u00f3w w przysz\u0142ych okresach.</li> <li>Analiza sieciowa (graph analysis) \u2013 optymalizacja tras na podstawie danych OSM i sieci dr\u00f3g.</li> </ul> </li> </ul>"},{"location":"projects/anomalies_sentinel2/","title":"Wykrywanie anomalii na obrazach Sentinel-2","text":"<p>Opis problemu badawczego: Dane optyczne pozwalaj\u0105 na monitorowanie zmian w \u015brodowisku, ale detekcja nietypowych zdarze\u0144 (anomalii) wymaga zaawansowanej analizy danych. Anomalie mog\u0105 obejmowa\u0107 wylesienia, susze, po\u017cary, zmiany pokrycia terenu, zanieczyszczenia w\u00f3d i anomalia w ro\u015blinno\u015bci. Celem projektu jest opracowanie systemu automatycznego wykrywania anomalii na podstawie wska\u017anik\u00f3w spektralnych i metod uczenia maszynowego, w tym algorytm\u00f3w nadzorowanych i nienadzorowanych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>Microsoft Planetary Computer</li> <li>Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>Google Earth Engine</li> <li>Copernicus Data Space Ecosystem</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pne przetwarzanie - korekcja atmosferyczna, maskowanie chmur, przyci\u0119cie do AOI</li> <li>Ekstrakcja cech spektralnych - obliczenie wska\u017anik\u00f3w spektralnych (np. NDVI czy MSAVI - zmiany wegetacji, NDWI - zmiany dot. zasi\u0119gu w\u00f3d powierzchniowych, NBR - wykrywanie po\u017car\u00f3w)</li> <li>Wykrywanie anomalii<ul> <li>Metody statystyczne (regu\u0142y odstaj\u0105ce):<ul> <li>Z-score \u2013 oznaczanie warto\u015bci odleg\u0142ych od \u015bredniej o wi\u0119cej ni\u017c X odchyle\u0144 standardowych</li> <li>Percentylowa analiza progowa \u2013 wykrywanie warto\u015bci powy\u017cej 95. lub poni\u017cej 5. percentyla dla danego obszaru i wska\u017anika spektralnego</li> <li>Metody bazuj\u0105ce na g\u0142\u00f3wnych sk\u0142adowych (PCA) \u2013 analiza kierunk\u00f3w najwi\u0119kszej wariancji i identyfikacja punkt\u00f3w odstaj\u0105cych</li> </ul> </li> <li>Metody wykrywania anomalii oparte na ML (unsupervised learning):<ul> <li>Isolation Forest \u2013 model izoluj\u0105cy nietypowe obserwacje (np. nag\u0142e zmiany NDVI po wylesieniu).</li> <li>Local Outlier Factor (LOF) \u2013 wykrywanie anomalii poprzez analiz\u0119 g\u0119sto\u015bci punkt\u00f3w w przestrzeni cech spektralnych.</li> <li>DBSCAN \u2013 grupowanie anomalii na podstawie g\u0119sto\u015bci obserwacji (pozwala wykrywa\u0107 lokalne zmiany w \u015brodowisku).</li> <li>Autoenkodery (AE) \u2013 redukcja wymiarowo\u015bci i wykrywanie nietypowych pikseli poprzez r\u00f3\u017cnic\u0119 mi\u0119dzy rekonstrukcj\u0105 a rzeczywistym obrazem.</li> </ul> </li> </ul> </li> </ul>"},{"location":"projects/insar/","title":"Wykrywanie zmian wysoko\u015bci terenu na podstawie danych SAR","text":"<p>Opis problemu badawczego: Zmiany wysoko\u015bci terenu mog\u0105 by\u0107 wynikiem zjawisk geologicznych (trz\u0119sienia ziemi, wulkany, osuwiska), dzia\u0142a\u0144 cz\u0142owieka (eksploatacja g\u00f3rnicza, zapadliska, urbanizacja) lub proces\u00f3w naturalnych (erozja, topnienie lodowc\u00f3w). Celem projektu jest monitorowanie przemieszcze\u0144 powierzchni ziemi w czasie, z wykorzystaniem danych SAR z r\u00f3\u017cnych misji radarowych. Analiza b\u0119dzie oparta na technikach interferometrycznych (InSAR).</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>Sentinel-1 poprzez Copernicus Data Space Ecosystem / Microsoft Planetary Computer / Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>Programy otwartych danych od komercyjnych dostawc\u00f3w (ICEYE, Umbra, Capella Space)</li> </ul> <p>Dodatkowe informacje:</p> <ul> <li>Biblioteki dedykowane dla analiz na danych SAR to m.in. <code>PyGMTSAR</code> czy <code>MintPy</code></li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wyb\u00f3r analizowanego epizodu (np. wulkan Fernandina - Gal\u00e1pagos Islands, Ecuador)</li> <li>Korekcja geometryczna i radiometryczna</li> <li>Tworzenie interferogram\u00f3w \u2013 por\u00f3wnanie fazy fali radarowej</li> <li>Persistent Scatterer Interferometry (PS-InSAR) \u2013 analiza d\u0142ugoterminowych zmian wysoko\u015bci</li> <li>Small Baseline Subset (SBAS) \u2013 kr\u00f3tkoterminowe zmiany</li> <li>Konwersja r\u00f3\u017cnic fazowych na warto\u015bci wysoko\u015bciowe</li> <li>Wizualizacja - mapy deformacji, wykresy zmian</li> </ul>"},{"location":"projects/lulc/","title":"Analiza zmian w u\u017cytkowaniu terenu dla wybranego obszaru na podstawie gotowych klasyfikacji","text":"<p>Opis problemu badawczego: Zmiany w u\u017cytkowaniu terenu odgrywaj\u0105 kluczow\u0105 rol\u0119 w planowaniu przestrzennym, zarz\u0105dzaniu zasobami naturalnymi i analizie wp\u0142ywu cz\u0142owieka na \u015brodowisko. Dzi\u0119ki gotowym produktom klasyfikacji u\u017cytkowania terenu, mo\u017cliwa jest analiza d\u0142ugoterminowych trend\u00f3w zmian krajobrazu. Celem projektu jest identyfikacja zmian w u\u017cytkowaniu terenu w wybranym regionie na podstawie gotowych danych klasyfikacyjnych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>CORINE Land Cover (CLC)</li> <li>Copernicus Global Land Cover</li> <li>ESA WorldCover</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Statystyczna analiza trend\u00f3w u\u017cytkowania terenu \u2013 obliczenie powierzchni poszczeg\u00f3lnych klas dla r\u00f3\u017cnych lat</li> <li>Wizualizacja zmian w postaci map i wykres\u00f3w \u2013 interaktywne mapy r\u00f3\u017cnic u\u017cytkowania terenu</li> <li>Okre\u015blenie hotspot\u00f3w - obszar\u00f3w, gdzie wyst\u0119puje du\u017ca ilo\u015b\u0107 zmian</li> <li>?Modelowanie prawdopodobie\u0144stwa konwersji terenu na podstawie historycznych zmian?</li> </ul>"},{"location":"projects/meteo_data_for_cities/","title":"Zmiany klimatyczne w miastach - analiza trend\u00f3w w danych z Copernicus Climate Data Store","text":"<p>Opis problemu badawczego: Zmiany klimatu wp\u0142ywaj\u0105 na warunki atmosferyczne w miastach, wzmacniaj\u0105c efekt miejskiej wyspy ciep\u0142a (UHI) i zmieniaj\u0105c wzorce opad\u00f3w czy pr\u0119dko\u015bci wiatru. Celem projektu jest analiza d\u0142ugoterminowych trend\u00f3w wybranych parametr\u00f3w meteorologicznych (np. temperatura, opady, pr\u0119dko\u015b\u0107 wiatru) w wybranych aglomeracjach na podstawie wybranych \u017ar\u00f3de\u0142 danych. Badanie pozwoli okre\u015bli\u0107 tempo zmian oraz ich sezonowo\u015b\u0107, a tak\u017ce por\u00f3wna\u0107 miasta o r\u00f3\u017cnych warunkach klimatycznych.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>ERA-5 daily statistics - CDS</li> <li>Inne, znalezione samodzielnie</li> </ul> <p>Dodatkowe informacje:</p> <ul> <li>CDS posiada w\u0142asne API</li> <li>ECMWF posiada w\u0142asny projekt earthkit, kt\u00f3ry niby (nie korzysta\u0142em z niego) u\u0142atwia wczytywanie i prac\u0119 z danymi</li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Wst\u0119pne czyszczenie i agregacja danych szereg\u00f3w czasowych (np. u\u015brednianie miesi\u0119czne lub roczne)</li> <li>Eksploracyjna analiza danych (EDA) - wykresy trend\u00f3w w czasie, wykresy korelacji</li> <li>Analiza statystyczna trendu (np. dopasowanie liniowej regresji trendu lub krzywej nieliniowej)</li> <li>Modelowanie szereg\u00f3w czasowych - zastosowanie modeli prognostycznych (np. ARIMA, Prophet lub LSTM) do przewidywania przysz\u0142ych warto\u015bci</li> <li>Ocena niepewno\u015bci prognoz i walidacja modelu na danych historycznych (podzia\u0142 na zbi\u00f3r treningowy i testowy z ostatnich lat)</li> </ul>"},{"location":"projects/other/","title":"Inne","text":""},{"location":"projects/other/#analiza-danych-rastrowych","title":"Analiza danych rastrowych","text":"<p>Przyk\u0142adowe projekty z kt\u00f3rych mo\u017cna czerpa\u0107 inspiracj\u0119:</p> <ul> <li>Segment Anything Model by Qiusheng Wu</li> </ul>"},{"location":"projects/other/#analiza-otwartych-danych-wektorowych","title":"Analiza otwartych danych wektorowych","text":"<p>Jako projekt mo\u017cna r\u00f3wnie\u017c wybra\u0107 interesuj\u0105ce warstwy z otwartych danych wektorowych (Open Street Maps czy Overture Maps) i przeanalizowa\u0107 je w wybrany spos\u00f3b.</p> <p>Przyk\u0142adowe projekty z kt\u00f3rych mo\u017cna czerpa\u0107 inspiracj\u0119:</p> <ul> <li>City-Summit by Kamil Raczycki - takiego typu charakterystyk\u0119 mo\u017cna zrobi\u0107 te\u017c na innych warstwach dost\u0119pnych</li> </ul>"},{"location":"projects/other/#projekty-bardziej-zaawansowane-technicznie-ale-pokrywajace-mniej-obszarow-pracy-z-danymi","title":"Projekty bardziej zaawansowane technicznie, ale pokrywaj\u0105ce mniej obszar\u00f3w pracy z danymi","text":"<p>Istnieje tak\u017ce mo\u017cliwo\u015b\u0107 stworzenia projektu skupiaj\u0105cego si\u0119 na jednym z komponent\u00f3w analizy danych, np. przeszukiwaniu katalog\u00f3w danych, maskowaniu chmur z wykorzystaniem r\u00f3\u017cnych algorytm\u00f3w czy rozproszonym przetwarzaniu danych przestrzennych. W takim wypadku szczeg\u00f3\u0142y zostan\u0105 uzgodnione indywidualnie z grup\u0105.</p>"},{"location":"projects/sr/","title":"Por\u00f3wnanie dost\u0119pnych modeli Super Resolution dla danych Sentinel-2","text":"<p>Opis problemu badawczego: Obrazy satelitarne Sentinel-2 oferuj\u0105 wielospektralne zobrazowania w r\u00f3\u017cnych rozdzielczo\u015bciach (10 m, 20 m, 60 m), ale nie wszystkie pasma maj\u0105 wysok\u0105 rozdzielczo\u015b\u0107 przestrzenn\u0105. To ogranicza ich wykorzystanie w precyzyjnych analizach \u015brodowiskowych, takich jak detekcja zmian, klasyfikacja pokrycia terenu czy monitoring urbanizacji. Celem projektu jest zastosowanie technik Super Resolution (SR) do poprawy jako\u015bci danych Sentinel-2, wykorzystuj\u0105c otwarte modele g\u0142\u0119bokiego uczenia do zwi\u0119kszania rozdzielczo\u015bci obraz\u00f3w.</p> <p>Proponowane \u017ar\u00f3d\u0142a danych:</p> <ul> <li>W przypadku Sentinel-2 - Copernicus Data Space Ecosystem / Microsoft Planetary Computer / Earth Search by Element 84 (AWS Registry of Open Data)</li> <li>W przypadku modeli do SR:<ul> <li>DSen2</li> <li>ESRGAN, SRCNN, HighResNet od AllenAI</li> <li>Sentinel-2 Deep Resolution (S2DR2 / S2DR3)</li> <li>Inne</li> </ul> </li> </ul> <p>Proponowane techniki analizy danych:</p> <ul> <li>Pobranie obraz\u00f3w Sentinel-2</li> <li>Zastosowanie gotowych modeli</li> <li>Wizualne por\u00f3wnanie i analiza wybranego wska\u017anika spektralnego</li> </ul>"},{"location":"projects/ways_of_working/","title":"Spos\u00f3b pracy nad projektami","text":""},{"location":"projects/ways_of_working/#ogolne-zasady-pracy-nad-projektami","title":"\ud83d\udccc Og\u00f3lne zasady pracy nad projektami","text":"<ol> <li> <p>Wszystkie analizy prowadzimy w Pythonie</p> <ul> <li>Mo\u017cliwe jest zar\u00f3wno budowanie pakietu i uruchamianie analiz przez CLI jak i stworzenie przejrzystych Jupyter Notebooks.</li> <li>\u015arodowisko wirtualne musi by\u0107 odtwarzalne, a wi\u0119c konieczne jest dodanie jego definicji do repozytorium.</li> </ul> </li> <li> <p>Dokumentacja</p> <ul> <li>Ostateczne wyniki projektu powinny zosta\u0107 udost\u0119pnione w formie dokumentacji w GitHub Pages (mo\u017cliwa inna forma).</li> <li>Nale\u017cy prowadzi\u0107 cotygodniowy dziennik (log), gdzie b\u0119d\u0105 2 sekcje: 1) osi\u0105gniety post\u0119p oraz 2) napotkanie problemy / kwestie do przedyskutowania podczas zaj\u0119\u0107.</li> <li>Plik <code>README.md</code> powinien zawiera\u0107 kroki, kt\u00f3re nale\u017cy podj\u0105\u0107 aby uruchomi\u0107 analizy.</li> </ul> </li> <li> <p>Praca podczas zaj\u0119\u0107</p> <ul> <li>Przed ka\u017cdym zaj\u0119ciami nale\u017cy mie\u0107 gotowy log (patrz wy\u017cej), na jego podstawie b\u0119dziemy prowadzi\u0107 dyskusj\u0119 co dalej z projektem.</li> </ul> </li> <li>Pracujemy w grupach 3-osobowych</li> </ol>"},{"location":"projects/ways_of_working/#harmonogram","title":"\ud83d\udcc5 Harmonogram","text":"<pre><code>- Marzec:\n    - 21 - Om\u00f3wienie i wybranie temat\u00f3w\n    - 28 - Przedstawienie wybranych \u017ar\u00f3de\u0142 danych i koncepcji projektu\n- Kwiecie\u0144:\n    - 04 + 11 - Dost\u0119p do danych (pobieranie, wczytywanie)\n    - 25 + 30 - Przygotowane dane do przeprowadzenia analizy (czyszczenie, przygotowanie i eksploracja)\n- Maj:\n    - 09 + 16 + 23 - Modelowanie / analizy statystyczne / ML\n- Czerwiec:\n    - 30 + 06 - Wyniki (wizualizacja)\n    - 13 + 25 - Opracowanie rezultat\u00f3w projektu i wystawienie ocen\n</code></pre>"}]}